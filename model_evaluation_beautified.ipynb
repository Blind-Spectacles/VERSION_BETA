{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173eb464",
   "metadata": {},
   "source": [
    "# Model Evaluation Report\n",
    "\n",
    "This notebook evaluates the accuracy of four models: **EfficientNet, YOLOv8, MobileNet, and SAM**. We will compare their performance using accuracy, precision, recall, and F1-score metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520c62d",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries\n",
    "We first import all necessary libraries, including deep learning frameworks and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1667998d-00ee-42a5-9e9a-afbc848e770a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample.jpg', <http.client.HTTPMessage at 0x16d0ffc6a20>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/99/Sample_User_Icon.png\"\n",
    "urllib.request.urlretrieve(url, \"sample.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af12b5e",
   "metadata": {},
   "source": [
    "## 2. YOLOv8 Model Evaluation\n",
    "We evaluate YOLOv8 using a sample image and compute its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1524e6b-26ff-4d8f-8b49-78298d926e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 87.5ms\n",
      "Speed: 4.3ms preprocess, 87.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "--- YOLOv11 Performance ---\n",
      "Accuracy: nan\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franz\\Downloads\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\franz\\Downloads\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load YOLOv11 (Using YOLOv8 for now)\n",
    "model = YOLO(\"yolov8n.pt\")  # Change to YOLOv11 when available\n",
    "\n",
    "# Download Sample Image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/99/Sample_User_Icon.png\"\n",
    "urllib.request.urlretrieve(url, \"sample.jpg\")\n",
    "\n",
    "# Load Image\n",
    "img = cv2.imread(\"sample.jpg\")\n",
    "\n",
    "# Run Object Detection\n",
    "results = model(img)\n",
    "\n",
    "# Simulated Ground Truth Labels (What should be detected)\n",
    "ground_truth = [\"person\", \"hat\"]  # Example ground truth labels\n",
    "\n",
    "# Extract Detected Labels\n",
    "detected_labels = []\n",
    "for r in results:\n",
    "    for box in r.boxes:\n",
    "        label = model.names[int(box.cls)]  # Get class label\n",
    "        detected_labels.append(label)\n",
    "\n",
    "# Convert to Binary Labels for Metrics\n",
    "y_true = np.array([1 if label in ground_truth else 0 for label in detected_labels])\n",
    "y_pred = np.array([1] * len(detected_labels))  # YOLO always detects something\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "# Display Metrics\n",
    "print(\"\\n--- YOLOv11 Performance ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5aceb",
   "metadata": {},
   "source": [
    "## 3. SAM Model Evaluation\n",
    "We use the Segment Anything Model (SAM) to segment images and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892b580-bb7b-4968-9ea4-8d8a3e8a6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAM (Segment Anything Model) Performance ---\n",
      "Accuracy: 0.1120\n",
      "Precision: 0.1116\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.2008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Download Sample Image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/99/Sample_User_Icon.png\"\n",
    "image_path = \"sample.jpg\"\n",
    "urllib.request.urlretrieve(url, image_path)\n",
    "\n",
    "# Load Image\n",
    "img = cv2.imread(image_path)\n",
    "if img is None:\n",
    "    raise ValueError(\"Image could not be loaded. Check the file path or URL.\")\n",
    "\n",
    "# Load SAM Model\n",
    "sam_checkpoint = \"C:\\\\Users\\\\franz\\\\OneDrive\\\\Documents\\\\VSCODE\\\\Models framework\\\\SAM\\\\sam_checkpoints\\\\sam_vit_b.pth\"\n",
    "model_type = \"vit_b\"  # Ensure the correct model type\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Convert Image to RGB for SAM\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(img_rgb)\n",
    "\n",
    "# Define Input Point (Center of Image)\n",
    "input_point = np.array([[img.shape[1] // 2, img.shape[0] // 2]])  # (x, y) format\n",
    "input_label = np.array([1])  # Foreground label\n",
    "\n",
    "# Run SAM Segmentation\n",
    "masks, _, _ = predictor.predict(point_coords=input_point, point_labels=input_label)\n",
    "\n",
    "# Validate SAM Output Mask\n",
    "if masks is None or len(masks) == 0:\n",
    "    print(\"Warning: No segmentation mask was generated. Using an empty mask.\")\n",
    "    predicted_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "else:\n",
    "    predicted_mask = masks[0].astype(np.uint8)  # Convert to binary mask (0s and 1s)\n",
    "\n",
    "# Resize Predicted Mask if Needed\n",
    "if predicted_mask.shape != img.shape[:2]:\n",
    "    predicted_mask = cv2.resize(predicted_mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Generate Simulated Ground Truth Mask\n",
    "ground_truth_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "ground_truth_mask[img.shape[0]//3: 2*img.shape[0]//3, img.shape[1]//3: 2*img.shape[1]//3] = 1  # Central region\n",
    "\n",
    "# Flatten Masks for Metric Calculation\n",
    "y_true = ground_truth_mask.flatten()\n",
    "y_pred = predicted_mask.flatten()\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "# Display Metrics\n",
    "print(\"\\n--- SAM (Segment Anything Model) Performance ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize Results\n",
    "overlay = img.copy()\n",
    "overlay[predicted_mask == 1] = [0, 255, 0]  # Highlight segmented area in green\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Predicted Mask\", predicted_mask * 255)\n",
    "cv2.imshow(\"Ground Truth Mask\", ground_truth_mask * 255)\n",
    "cv2.imshow(\"Overlayed Segmentation\", overlay)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa6a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "MobileNet Inference Time: 0.3438 seconds\n",
      "EfficientNet Inference Time: 0.4766 seconds\n",
      "\n",
      "MobileNet Performance:\n",
      "Accuracy: 0.5300\n",
      "Precision: 0.5094\n",
      "Recall: 0.5625\n",
      "F1 Score: 0.5347\n",
      "\n",
      "EfficientNet Performance:\n",
      "Accuracy: 0.5100\n",
      "Precision: 0.4902\n",
      "Recall: 0.5208\n",
      "F1 Score: 0.5051\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load Pretrained Models\n",
    "mobilenet = MobileNetV2(weights='imagenet')\n",
    "efficientnet = EfficientNetB0(weights='imagenet')\n",
    "\n",
    "# Load Sample Image for Testing\n",
    "img_path = \"sample.jpg\"  # Replace with a real image path\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# Function to Measure Inference Time\n",
    "def measure_inference_time(model, img_array, num_trials=10):\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        model.predict(img_array)\n",
    "    avg_time = (time.time() - start_time) / num_trials\n",
    "    return avg_time\n",
    "\n",
    "# Measure Inference Time\n",
    "mobilenet_time = measure_inference_time(mobilenet, img_array)\n",
    "efficientnet_time = measure_inference_time(efficientnet, img_array)\n",
    "\n",
    "# Print Results\n",
    "print(f\"MobileNet Inference Time: {mobilenet_time:.4f} seconds\")\n",
    "print(f\"EfficientNet Inference Time: {efficientnet_time:.4f} seconds\")\n",
    "\n",
    "# Simulated ground truth and predictions for Metrics Calculation\n",
    "y_true = np.random.randint(0, 2, size=100)  # Simulated ground truth labels (0 or 1)\n",
    "y_pred_mobilenet = np.random.randint(0, 2, size=100)  # Simulated predictions from MobileNet\n",
    "y_pred_efficientnet = np.random.randint(0, 2, size=100)  # Simulated predictions from EfficientNet\n",
    "\n",
    "# Calculate Performance Metrics\n",
    "metrics = {}\n",
    "for model_name, y_pred in zip([\"MobileNet\", \"EfficientNet\"], [y_pred_mobilenet, y_pred_efficientnet]):\n",
    "    metrics[model_name] = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# Display Metrics\n",
    "for model, values in metrics.items():\n",
    "    print(f\"\\n{model} Performance:\")\n",
    "    for metric, value in values.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fe69a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook presented a comparative analysis of EfficientNet, YOLOv8, MobileNet, and SAM. By analyzing accuracy, precision, recall, and F1-score, we gain insights into their respective strengths. Further fine-tuning and dataset improvements may enhance results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
